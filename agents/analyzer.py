import os
import pandas as pd
import numpy as np
from datetime import datetime
import json
import re
from typing import Dict, List, Any, Optional, Tuple
import warnings

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç scipy
try:
    from scipy import stats

    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    print("‚ö†Ô∏è SciPy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å numpy.")


class DataAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å AI –∞–Ω–∞–ª–∏–∑–æ–º —á–µ—Ä–µ–∑ OpenAI GPT"""

    def __init__(self):
        self.warnings = []

    def analyze(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            results = {
                'metrics': self._calculate_metrics(df),
                'trends': self._detect_trends(df),
                'patterns': self._find_patterns(df),
                'anomalies': self._detect_anomalies(df),
                'recommendations': self._generate_recommendations(df),
                'summary': self._create_summary(df)
            }
            return results
        except Exception as e:
            return {'error': str(e)}

    def _calculate_metrics(self, df: pd.DataFrame) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        metrics = {}

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics['total_records'] = len(df)
        metrics['total_columns'] = len(df.columns)

        # –ß–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            metrics['numeric_columns'] = len(numeric_cols)

            for col in numeric_cols[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 5 –∫–æ–ª–æ–Ω–∫–∞–º–∏
                metrics[f'{col}_mean'] = float(df[col].mean())
                metrics[f'{col}_median'] = float(df[col].median())
                metrics[f'{col}_std'] = float(df[col].std())
                metrics[f'{col}_sum'] = float(df[col].sum())
                metrics[f'{col}_min'] = float(df[col].min())
                metrics[f'{col}_max'] = float(df[col].max())

        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_total = df.isnull().sum().sum()
        metrics['missing_values'] = missing_total
        metrics['missing_percentage'] = float(missing_total / (len(df) * len(df.columns)) * 100)

        return metrics

    def _detect_trends(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤"""
        trends = []

        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏
        date_cols = df.select_dtypes(include=['datetime64']).columns

        if len(date_cols) > 0:
            date_col = date_cols[0]
            numeric_cols = df.select_dtypes(include=[np.number]).columns

            for col in numeric_cols:
                if len(df) > 1:
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º numpy –µ—Å–ª–∏ scipy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                        if SCIPY_AVAILABLE:
                            slope, intercept, r_value, p_value, std_err = stats.linregress(
                                range(len(df)), df[col].fillna(df[col].mean())
                            )
                        else:
                            # –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —á–µ—Ä–µ–∑ numpy
                            x = np.arange(len(df))
                            y = df[col].fillna(df[col].mean()).values
                            A = np.vstack([x, np.ones(len(x))]).T
                            slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]
                            r_value = np.corrcoef(x, y)[0, 1]
                            p_value = None
                            std_err = np.std(y - (slope * x + intercept))

                        direction = "—Ä–æ—Å—Ç" if slope > 0 else "—Å–Ω–∏–∂–µ–Ω–∏–µ"
                        strength = "—Å–∏–ª—å–Ω—ã–π" if abs(r_value) > 0.7 else "—É–º–µ—Ä–µ–Ω–Ω—ã–π" if abs(r_value) > 0.3 else "—Å–ª–∞–±—ã–π"

                        trends.append({
                            '–ú–µ—Ç—Ä–∏–∫–∞': col,
                            '–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ': direction,
                            '–°–∏–ª–∞': strength,
                            '–ù–∞–∫–ª–æ–Ω': float(slope),
                            'R-–∫–≤–∞–¥—Ä–∞—Ç': float(r_value ** 2),
                            '–ó–Ω–∞—á–∏–º–æ—Å—Ç—å': "–∑–Ω–∞—á–∏–º" if (p_value is None or p_value < 0.05) else "–Ω–µ–∑–Ω–∞—á–∏–º"
                        })
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è {col}: {e}")
                        continue

        return trends

    def _find_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        patterns = {}

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            correlations = {}
            for i in range(len(numeric_cols)):
                for j in range(i + 1, len(numeric_cols)):
                    col1, col2 = numeric_cols[i], numeric_cols[j]
                    try:
                        if SCIPY_AVAILABLE:
                            corr, p_value = stats.pearsonr(
                                df[col1].fillna(df[col1].mean()),
                                df[col2].fillna(df[col2].mean())
                            )
                        else:
                            corr = np.corrcoef(
                                df[col1].fillna(df[col1].mean()),
                                df[col2].fillna(df[col2].mean())
                            )[0, 1]
                            p_value = None

                        if abs(corr) > 0.7:
                            correlations[f'{col1}_{col2}'] = {
                                'correlation': float(corr),
                                'strength': '—Å–∏–ª—å–Ω–∞—è',
                                'significance': '–∑–Ω–∞—á–∏–º–∞—è' if (p_value is None or p_value < 0.05) else '–Ω–µ–∑–Ω–∞—á–∏–º–∞—è'
                            }
                    except:
                        continue

            patterns['correlations'] = correlations

        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
        date_cols = df.select_dtypes(include=['datetime64']).columns
        if len(date_cols) > 0:
            date_col = date_cols[0]
            patterns['has_dates'] = True
            patterns['date_range'] = {
                'start': str(df[date_col].min()),
                'end': str(df[date_col].max()),
                'duration_days': (df[date_col].max() - df[date_col].min()).days
            }

        return patterns

    def _detect_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
        anomalies = {}

        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            try:
                values = df[col].dropna()
                if len(values) > 10:
                    mean = values.mean()
                    std = values.std()

                    # –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ 3-—Å–∏–≥–º
                    lower_bound = mean - 3 * std
                    upper_bound = mean + 3 * std

                    outlier_count = ((values < lower_bound) | (values > upper_bound)).sum()
                    if outlier_count > 0:
                        anomalies[col] = {
                            'outlier_count': int(outlier_count),
                            'percentage': float(outlier_count / len(values) * 100),
                            'mean': float(mean),
                            'std': float(std),
                            'lower_bound': float(lower_bound),
                            'upper_bound': float(upper_bound)
                        }
            except:
                continue

        return anomalies

    def _generate_recommendations(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        missing_percentage = (df.isnull().sum() / len(df) * 100)
        high_missing = missing_percentage[missing_percentage > 20].index.tolist()

        if high_missing:
            recommendations.append({
                'type': 'data_quality',
                'priority': 'high',
                'text': f'–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö: {", ".join(high_missing[:3])}'
            })

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω—ã–º
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) > 0:
            for col in numeric_cols[:3]:
                if df[col].std() / df[col].mean() > 0.5:
                    recommendations.append({
                        'type': 'data_variability',
                        'priority': 'medium',
                        'text': f'–í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –≤ {col}. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.'
                    })

        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(df) > 1000:
            recommendations.append({
                'type': 'performance',
                'priority': 'low',
                'text': '–ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.'
            })

        return recommendations

    def _create_summary(self, df: pd.DataFrame) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ"""
        summary_parts = []

        summary_parts.append(f"–î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç {len(df)} –∑–∞–ø–∏—Å–µ–π –∏ {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫.")

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            summary_parts.append(f"–ù–∞–π–¥–µ–Ω–æ {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.")

        date_cols = df.select_dtypes(include=['datetime64']).columns
        if len(date_cols) > 0:
            summary_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã: {', '.join(date_cols[:3])}.")

        missing_total = df.isnull().sum().sum()
        if missing_total > 0:
            summary_parts.append(
                f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {missing_total} ({missing_total / (len(df) * len(df.columns)) * 100:.1f}%).")

        return " ".join(summary_parts)

    def gpt_analysis(self, df: pd.DataFrame = None, data_summary=None, trends=None, financial_metrics=None) -> str:
        """–†–µ–∞–ª—å–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ OpenAI GPT"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å OpenAI
            import openai

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key or api_key == "your_openai_api_key_here":
                return self._get_fallback_analysis(df, trends, financial_metrics)

            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
            client = openai.OpenAI(api_key=api_key)

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT
            prompt = self._create_gpt_prompt(df, data_summary, trends, financial_metrics)

            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system",
                            "content": """–¢—ã –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. 
                            –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å-–¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–≤–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
                            –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–¥–∞—É–Ω —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
                            –ë—É–¥—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º, –Ω–æ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–º. –í—ã–¥–µ–ª—è–π –∫–∞–∫ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã, —Ç–∞–∫ –∏ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è."""
                        },
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1500,
                    temperature=0.7,
                    top_p=0.9
                )

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç GPT
                return response.choices[0].message.content

            except openai.RateLimitError:
                return "‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI API. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            except openai.APITimeoutError:
                return "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É."
            except Exception as e:
                error_msg = str(e)
                if "insufficient_quota" in error_msg:
                    return "‚ö†Ô∏è –ó–∞–∫–æ–Ω—á–∏–ª—Å—è –±–∞–ª–∞–Ω—Å –Ω–∞ OpenAI API. –ü–æ–ø–æ–ª–Ω–∏—Ç–µ —Å—á–µ—Ç."
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ OpenAI API: {error_msg[:200]}"

        except ImportError:
            return self._get_fallback_analysis(df, trends, financial_metrics)

    def _create_gpt_prompt(self, df: pd.DataFrame, data_summary, trends, financial_metrics) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è GPT –∞–Ω–∞–ª–∏–∑–∞"""

        prompt_parts = ["# üîç –ê–ù–ê–õ–ò–ó –ë–ò–ó–ù–ï–°-–î–ê–ù–ù–´–•\n\n"]

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
        if df is not None:
            prompt_parts.append("## üìä –°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–•:")
            prompt_parts.append(f"- **–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö:** {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                prompt_parts.append(f"- **–ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ({len(numeric_cols)}):** {', '.join(numeric_cols[:5])}")
                if len(numeric_cols) > 5:
                    prompt_parts.append(f"  ... –∏ –µ—â–µ {len(numeric_cols) - 5} –∫–æ–ª–æ–Ω–æ–∫")

            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            if len(categorical_cols) > 0:
                prompt_parts.append(
                    f"- **–¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ({len(categorical_cols)}):** {', '.join(categorical_cols[:3])}")

            date_cols = df.select_dtypes(include=['datetime64']).columns
            if len(date_cols) > 0:
                for col in date_cols[:2]:
                    prompt_parts.append(
                        f"- **–í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ {col}:** —Å {df[col].min().date()} –ø–æ {df[col].max().date()}")

        # –°–≤–æ–¥–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if data_summary:
            prompt_parts.append("\n## üìà –û–°–ù–û–í–ù–ê–Ø –°–í–û–î–ö–ê:")
            if isinstance(data_summary, str):
                prompt_parts.append(data_summary)
            elif isinstance(data_summary, dict) and 'summary' in data_summary:
                prompt_parts.append(data_summary['summary'])

        # –¢—Ä–µ–Ω–¥—ã
        if trends and len(trends) > 0:
            prompt_parts.append("\n## üìà –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –¢–†–ï–ù–î–´:")
            for trend in trends[:5]:
                metric = trend.get('–ú–µ—Ç—Ä–∏–∫–∞', '–ú–µ—Ç—Ä–∏–∫–∞')
                direction = trend.get('–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å—Ç–∞–±–∏–ª—å–Ω—ã–π')
                strength = trend.get('–°–∏–ª–∞', '—Å—Ä–µ–¥–Ω–∏–π')
                r_squared = trend.get('R-–∫–≤–∞–¥—Ä–∞—Ç', 0)
                prompt_parts.append(f"- **{metric}:** {direction} ({strength}, R¬≤={r_squared:.3f})")

        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if financial_metrics:
            prompt_parts.append("\n## üí∞ –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
            for key, value in list(financial_metrics.items())[:8]:
                if isinstance(value, (int, float)):
                    if abs(value) >= 1000000:
                        display_value = f"{value / 1000000:.2f} –º–ª–Ω"
                    elif abs(value) >= 1000:
                        display_value = f"{value / 1000:.1f} —Ç—ã—Å"
                    else:
                        display_value = f"{value:,.0f}"

                    key_display = key.replace('_', ' ').title()
                    prompt_parts.append(f"- **{key_display}:** {display_value}")

        # –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        if df is not None and len(df) > 0:
            prompt_parts.append("\n## üéØ –ó–ê–î–ê–ß–ê –î–õ–Ø AI-–ê–ù–ê–õ–ò–¢–ò–ö–ê:")
        else:
            prompt_parts.append("\n## üéØ –ó–ê–î–ê–ß–ê –î–õ–Ø AI-–ê–ù–ê–õ–ò–¢–ò–ö–ê (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏):")

        prompt_parts.append("""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –±–∏–∑–Ω–µ—Å-–¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç:

### 1. –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´ (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ):
- –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –±–∏–∑–Ω–µ—Å–∞
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–∏—Å–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –ì–ª–∞–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–æ—Å—Ç–∞

### 2. –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
- **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ** (1-3 –º–µ—Å—è—Ü–∞): –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –±—ã—Å—Ç—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
- **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ** (6-12 –º–µ—Å—è—Ü–µ–≤): —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
- **–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:** –∫–∞–∫–∏–µ KPI –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å

### 3. –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø (—Ç–æ–ø-3):
1. –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
2. –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –≤–æ –≤—Ç–æ—Ä—É—é –æ—á–µ—Ä–µ–¥—å
3. –ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å –≤ —Ç—Ä–µ—Ç—å—é –æ—á–µ—Ä–µ–¥—å

### 4. –†–ò–°–ö–ò –ò –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:
- –ù–∞ —á—Ç–æ –æ–±—Ä–∞—Ç–∏—Ç—å –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
- –ß–µ–≥–æ —Å–ª–µ–¥—É–µ—Ç –∏–∑–±–µ–≥–∞—Ç—å
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### 5. –í–´–í–û–î–´:
- –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞
- –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–∏–∑–Ω–µ—Å–∞
- –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

**–§–æ—Ä–º–∞—Ç:** –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–¥–∞—É–Ω —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ ## –∏ ###, —Å–ø–∏—Å–∫–∏, –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–æ–≤.
**–¢–æ–Ω:** –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π, –ø–æ–ª–µ–∑–Ω—ã–π –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ –±–∏–∑–Ω–µ—Å–∞.
**–û–±—ä–µ–º:** –ü–æ–¥—Ä–æ–±–Ω—ã–π, –Ω–æ –±–µ–∑ –≤–æ–¥—ã. 800-1200 —Å–ª–æ–≤.
""")

        return "\n".join(prompt_parts)

    def _get_fallback_analysis(self, df, trends, financial_metrics) -> str:
        """–ó–∞–ø–∞—Å–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ OpenAI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""

        analysis_parts = ["# ü§ñ AI –ê–ù–ê–õ–ò–ó –ë–ò–ó–ù–ï–°-–î–ê–ù–ù–´–•\n"]
        analysis_parts.append("*‚ö†Ô∏è –†–µ–∂–∏–º –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (OpenAI API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω)*\n")

        if df is not None:
            analysis_parts.append(f"## üìä –ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú–´–ï –î–ê–ù–ù–´–ï")
            analysis_parts.append(f"- **–û–±—ä–µ–º:** {len(df)} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                analysis_parts.append(f"- **–ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:** {len(numeric_cols)} –∫–æ–ª–æ–Ω–æ–∫")
                top_numeric = numeric_cols[:3]
                for col in top_numeric:
                    mean_val = df[col].mean()
                    std_val = df[col].std()
                    analysis_parts.append(f"  ‚Ä¢ **{col}:** —Å—Ä–µ–¥–Ω–µ–µ = {mean_val:,.0f}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ = {std_val:,.0f}")

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            missing_total = df.isnull().sum().sum()
            if missing_total > 0:
                missing_pct = missing_total / (len(df) * len(df.columns)) * 100
                analysis_parts.append(f"- **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:** {missing_pct:.1f}% –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

        if trends and len(trends) > 0:
            analysis_parts.append("\n## üìà –û–°–ù–û–í–ù–´–ï –¢–†–ï–ù–î–´")
            for trend in trends[:3]:
                metric = trend.get('–ú–µ—Ç—Ä–∏–∫–∞', '–ú–µ—Ç—Ä–∏–∫–∞')
                direction = trend.get('–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å—Ç–∞–±–∏–ª—å–Ω—ã–π')
                analysis_parts.append(f"- **{metric}:** {direction}")

        analysis_parts.append("""
## üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò (–ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)

### 1. –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´:
- –ü—Ä–æ–≤–µ–¥–µ–Ω –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
- –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ —Ç—Ä–µ–Ω–¥—ã
- –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI API –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞

### 2. –î–ï–ô–°–¢–í–ò–Ø:
1. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ OpenAI API** –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
2. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ** –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ç—á–µ—Ç–æ–≤

### 3. –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:
- –î–æ–±–∞–≤—å—Ç–µ OPENAI_API_KEY –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
- –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ AI –∞–Ω–∞–ª–∏–∑
- –ü–æ–ª—É—á–∏—Ç–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç GPT

## üîß –ö–ê–ö –ù–ê–°–¢–†–û–ò–¢–¨ AI –ê–ù–ê–õ–ò–ó:
1. –ü–æ–ª—É—á–∏—Ç–µ API –∫–ª—é—á –Ω–∞ platform.openai.com
2. –î–æ–±–∞–≤—å—Ç–µ –≤ .env —Ñ–∞–π–ª: `OPENAI_API_KEY=–≤–∞—à_–∫–ª—é—á`
3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
""")

        return "\n".join(analysis_parts)

    def get_data_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞"""
        if df is None or df.empty:
            return {}

        return {
            'shape': df.shape,
            'dtypes': df.dtypes.astype(str).to_dict(),
            'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
            'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
            'date_columns': df.select_dtypes(include=['datetime64']).columns.tolist(),
            'missing_values': df.isnull().sum().to_dict(),
            'basic_stats': df.describe().to_dict() if len(df.select_dtypes(include=[np.number]).columns) > 0 else {},
            'summary': self._create_summary(df)
        }

    def basic_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è Telegram –±–æ—Ç–∞"""
        if df is None or df.empty:
            return {"error": "–ü—É—Å—Ç–æ–π DataFrame"}

        try:
            analysis = self.analyze(df)
            return analysis
        except Exception as e:
            return {"error": str(e)}