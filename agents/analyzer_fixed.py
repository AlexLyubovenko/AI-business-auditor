"""
–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è AI Business Auditor
–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI v1.6.1
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import os
from datetime import datetime
import json

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º OpenAI —Å –ü–ê–¢–ß–ï–ú
import openai
from openai import OpenAI as OriginalOpenAI


# –°–û–ó–î–ê–ï–ú –ü–ê–¢–ß–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ê–°–° Open–êI
class PatchedOpenAI(OriginalOpenAI):
    def __init__(self, api_key=None, **kwargs):
        # –£–î–ê–õ–Ø–ï–ú –í–°–ï –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        safe_kwargs = {k: v for k, v in kwargs.items()
                       if k not in ['proxies', 'api_base', 'organization',
                                    'timeout', 'max_retries', 'http_client']}
        super().__init__(api_key=api_key, **safe_kwargs)


# –ó–∞–º–µ–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
openai.OpenAI = PatchedOpenAI

# –¢–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π OpenAI
from openai import OpenAI


class DataAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""

    def __init__(self, api_key: Optional[str] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º—ã proxies"""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.client = None

        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataAnalyzer (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø). API –∫–ª—é—á: {'—É–∫–∞–∑–∞–Ω' if self.api_key else '–Ω–µ —É–∫–∞–∑–∞–Ω'}")

        if self.api_key and self.api_key.startswith("sk-"):
            try:
                # –°–ø–æ—Å–æ–± 1: –ë–ï–ó –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–æ–æ–±—â–µ
                self.client = OpenAI(api_key=self.api_key)
                print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Å–ø–æ—Å–æ–± 1)")

            except Exception as e1:
                print(f"‚ö†Ô∏è –°–ø–æ—Å–æ–± 1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e1}")

                try:
                    # –°–ø–æ—Å–æ–± 2: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–ª—é—á –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–µ–º –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    os.environ["OPENAI_API_KEY"] = self.api_key
                    self.client = OpenAI()  # –ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Å–ø–æ—Å–æ–± 2)")

                except Exception as e2:
                    print(f"‚ùå –°–ø–æ—Å–æ–± 2 —Ç–∞–∫–∂–µ –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è: {e2}")

                    try:
                        # –°–ø–æ—Å–æ–± 3: –£–ª—å—Ç—Ä–∞-–º–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π
                        self.client = OpenAI(
                            api_key=self.api_key,
                            # –¢–û–õ–¨–ö–û –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∏–∫–∞–∫–∏—Ö proxies!
                        )
                        print("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Å–ø–æ—Å–æ–± 3)")
                    except Exception as e3:
                        print(f"‚ùå –í—Å–µ —Å–ø–æ—Å–æ–±—ã –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å: {e3}")
                        self.client = None
        else:
            if self.api_key:
                print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç API –∫–ª—é—á–∞: {self.api_key[:10]}...")
            else:
                print("‚ÑπÔ∏è OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–Ω–µ—Ç –∫–ª—é—á–∞)")

    # –û–°–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ –û–°–¢–ê–Æ–¢–°–Ø –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô (–∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏)
    def analyze_dataframe(self, df: pd.DataFrame, analysis_type: str = "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π") -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ DataFrame —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
        try:
            analysis = {
                "timestamp": datetime.now().isoformat(),
                "analysis_type": analysis_type,
                "basic_stats": {},
                "financial_metrics": {},
                "trends": {},
                "anomalies": [],
                "summary": ""
            }

            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            text_cols = df.select_dtypes(include=['object']).columns.tolist()

            if numeric_cols:
                # –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                desc_stats = df[numeric_cols].describe().to_dict()

                simplified_stats = {}
                for col in numeric_cols[:10]:
                    if col in desc_stats:
                        simplified_stats[col] = {
                            "mean": float(desc_stats[col].get('mean', 0)),
                            "std": float(desc_stats[col].get('std', 0)),
                            "min": float(desc_stats[col].get('min', 0)),
                            "max": float(desc_stats[col].get('max', 0)),
                            "median": float(df[col].median() if not df[col].empty else 0)
                        }

                analysis["basic_stats"] = {
                    "numeric_summary": simplified_stats,
                    "total_numeric_columns": len(numeric_cols)
                }

                analysis["financial_metrics"] = self._calculate_financial_metrics(df)
                analysis["trends"] = self._detect_trends(df)
                analysis["anomalies"] = self._detect_anomalies(df)

            if text_cols:
                analysis["basic_stats"]["text_summary"] = {
                    "text_columns": text_cols,
                    "sample_values": {col: df[col].dropna().iloc[:3].tolist() if len(df[col].dropna()) > 0 else []
                                      for col in text_cols[:3]}
                }

            analysis["summary"] = self._generate_summary(analysis, len(df))
            return analysis

        except Exception as e:
            return {
                "timestamp": datetime.now().isoformat(),
                "analysis_type": analysis_type,
                "error": str(e),
                "summary": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
            }

    def _calculate_financial_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        metrics = {}

        revenue_keywords = ['–≤—ã—Ä—É—á–∫–∞', 'revenue', '–¥–æ—Ö–æ–¥', 'sales', 'income']
        cost_keywords = ['—Ä–∞—Å—Ö–æ–¥', 'cost', 'expense', '–∑–∞—Ç—Ä–∞—Ç', '–∏–∑–¥–µ—Ä–∂']
        profit_keywords = ['–ø—Ä–∏–±—ã–ª—å', 'profit', '–º–∞—Ä–∂', 'margin']

        revenue_cols = []
        cost_cols = []
        profit_cols = []

        for col in df.columns:
            col_lower = str(col).lower()

            if any(keyword in col_lower for keyword in revenue_keywords):
                revenue_cols.append(col)
            elif any(keyword in col_lower for keyword in cost_keywords):
                cost_cols.append(col)
            elif any(keyword in col_lower for keyword in profit_keywords):
                profit_cols.append(col)

        if revenue_cols:
            revenue_col = revenue_cols[0]
            total_revenue = df[revenue_col].sum()
            avg_revenue = df[revenue_col].mean()

            metrics["total_revenue"] = float(total_revenue)
            metrics["avg_revenue"] = float(avg_revenue)

            if len(df) > 1:
                try:
                    growth = ((df[revenue_col].iloc[-1] - df[revenue_col].iloc[0]) /
                              df[revenue_col].iloc[0] * 100)
                    metrics["revenue_growth_percent"] = float(growth)
                except:
                    metrics["revenue_growth_percent"] = None

        if cost_cols and revenue_cols:
            cost_col = cost_cols[0]
            revenue_col = revenue_cols[0]

            total_cost = df[cost_col].sum()
            total_revenue = df[revenue_col].sum()

            metrics["total_cost"] = float(total_cost)
            metrics["gross_profit"] = float(total_revenue - total_cost)

            if total_revenue > 0:
                metrics["gross_margin_percent"] = float(
                    (total_revenue - total_cost) / total_revenue * 100
                )

        if profit_cols:
            profit_col = profit_cols[0]
            metrics["total_profit"] = float(df[profit_col].sum())
            metrics["avg_profit"] = float(df[profit_col].mean())

        return metrics

    def _detect_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        trends = {}
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        for col in numeric_cols[:5]:
            if len(df[col]) > 2:
                try:
                    y_series = df[col].ffill()
                    y = y_series.values

                    if len(y) > 1 and not np.all(y == y[0]):
                        x = np.arange(len(y))
                        slope = np.polyfit(x, y, 1)[0]

                        if slope > 0.1:
                            trend_direction = "—Ä–æ—Å—Ç"
                        elif slope < -0.1:
                            trend_direction = "—Å–Ω–∏–∂–µ–Ω–∏–µ"
                        else:
                            trend_direction = "—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"

                        trends[col] = {
                            "direction": trend_direction,
                            "slope": float(slope),
                            "strength": "—Å–∏–ª—å–Ω—ã–π" if abs(slope) > 0.5 else "—É–º–µ—Ä–µ–Ω–Ω—ã–π" if abs(slope) > 0.1 else "—Å–ª–∞–±—ã–π"
                        }
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                    continue

        return trends

    def _detect_anomalies(self, df: pd.DataFrame) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö"""
        anomalies = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        for col in numeric_cols[:5]:
            if len(df[col]) > 10:
                try:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1

                    if IQR > 0:
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        anomaly_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index

                        for idx in anomaly_indices[:10]:
                            anomalies.append({
                                "column": col,
                                "row_index": int(idx) if pd.notnull(idx) else str(idx),
                                "value": float(df.loc[idx, col]),
                                "bounds": {
                                    "lower": float(lower_bound),
                                    "upper": float(upper_bound)
                                },
                                "deviation": "–Ω–∏–∂–µ" if df.loc[idx, col] < lower_bound else "–≤—ã—à–µ"
                            })
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                    continue

        return anomalies[:20]

    def _generate_summary(self, analysis: Dict, row_count: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞"""
        summary_parts = []

        summary_parts.append(f"–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞: {analysis.get('analysis_type', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        summary_parts.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–æ–∫: {row_count}")

        metrics = analysis.get("financial_metrics", {})
        if metrics:
            if "total_revenue" in metrics:
                summary_parts.append(f"–û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞: {metrics['total_revenue']:,.0f}")
            if "gross_profit" in metrics:
                summary_parts.append(f"–í–∞–ª–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å: {metrics['gross_profit']:,.0f}")
            if "gross_margin_percent" in metrics:
                summary_parts.append(f"–í–∞–ª–æ–≤–∞—è –º–∞—Ä–∂–∞: {metrics['gross_margin_percent']:.1f}%")

        trends = analysis.get("trends", {})
        if trends:
            trend_count = len([t for t in trends.values() if t.get("direction") == "—Ä–æ—Å—Ç"])
            if trend_count > 0:
                summary_parts.append(f"–í—ã—è–≤–ª–µ–Ω–æ {trend_count} —Ä–∞—Å—Ç—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤")

        anomalies = analysis.get("anomalies", [])
        if anomalies:
            summary_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö")

        return " | ".join(summary_parts)

    def generate_llm_insights(self, analysis_results: Dict, prompt_template: str = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.client:
            print("‚ùå OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return {
                "llm_analysis": {
                    "insights": ["‚ö†Ô∏è API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏–ª–∏ –Ω–µ–≤–µ—Ä–µ–Ω"],
                    "recommendations": ["‚úÖ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"],
                    "risks": []
                },
                "llm_used": False,
                "error": "API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
            }

        try:
            print("ü§ñ –ó–∞–ø—É—Å–∫–∞—é GPT-–∞–Ω–∞–ª–∏–∑...")

            context = {
                "analysis_type": analysis_results.get("analysis_type", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                "financial_metrics": analysis_results.get("financial_metrics", {}),
                "trends": analysis_results.get("trends", {}),
                "anomalies_count": len(analysis_results.get("anomalies", [])),
                "summary": analysis_results.get("summary", "")
            }

            prompt = prompt_template or f"""
            –¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ:

            –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {context['analysis_type']}
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª–∏–π: {context['anomalies_count']}

            –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
            {json.dumps(context['financial_metrics'], ensure_ascii=False, indent=2)}

            –¢—Ä–µ–Ω–¥—ã:
            {json.dumps(context['trends'], ensure_ascii=False, indent=2)}

            –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
            1. 3 –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–∞
            2. 2 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            3. 2 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–∞

            –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ JSON:
            {{
                "insights": ["–≤—ã–≤–æ–¥1", "–≤—ã–≤–æ–¥2", "–≤—ã–≤–æ–¥3"],
                "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è2"],
                "risks": ["—Ä–∏—Å–∫1", "—Ä–∏—Å–∫2"]
            }}
            """

            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "–¢—ã –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )

            llm_response = response.choices[0].message.content
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç OpenAI")

            try:
                llm_data = json.loads(llm_response)

                insights = []
                for insight in llm_data.get("insights", []):
                    if isinstance(insight, str) and insight.strip():
                        insights.append(insight.strip())

                recommendations = []
                for rec in llm_data.get("recommendations", []):
                    if isinstance(rec, str) and rec.strip():
                        recommendations.append(rec.strip())

                risks = []
                for risk in llm_data.get("risks", []):
                    if isinstance(risk, str) and risk.strip():
                        risks.append(risk.strip())

                return {
                    "llm_analysis": {
                        "insights": insights[:3],
                        "recommendations": recommendations[:2],
                        "risks": risks[:2]
                    },
                    "model_used": "gpt-3.5-turbo",
                    "tokens_used": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                    "llm_used": True
                }

            except json.JSONDecodeError:
                return {
                    "llm_analysis": {
                        "insights": [llm_response[:200] + "..."],
                        "recommendations": ["–û—Ç–≤–µ—Ç –Ω–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"],
                        "risks": ["–ü—Ä–æ–±–ª–µ–º–∞ —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –æ—Ç–≤–µ—Ç–∞"]
                    },
                    "llm_used": False,
                    "error": "–û—Ç–≤–µ—Ç –Ω–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"
                }

        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {error_msg}")

            return {
                "llm_analysis": {
                    "insights": [f"–û—à–∏–±–∫–∞: {error_msg[:100]}"],
                    "recommendations": ["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á", "–ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É"],
                    "risks": ["–ü—Ä–æ–±–ª–µ–º–∞ —Å AI-–∞–Ω–∞–ª–∏–∑–æ–º"]
                },
                "llm_used": False,
                "error": error_msg
            }